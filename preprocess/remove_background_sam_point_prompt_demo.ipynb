{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanchuangyi/miniconda3/envs/CV/lib/python3.10/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed A*5wfcQaqfSZtvQ0hKmwjDrwAAAQAAAQ.jpg\n",
      "Processed A*RUTcRqGoadQAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*yhhMRIVoNgIAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*7DHLR7A9-PcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*B0UFQY5-NWAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*Ilm_R4vQBwAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*AcLoQ62XAiEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*7LqiQ6pid-MAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*zO1xSLQ2HekAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*mn91SptgaVkAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*Vj18TaEJwkYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*XAoET6NgEiEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*wsZqTbPRGS8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*6rpyT7SHRbEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Processed A*qpEoQKj_Er0AAAAAAAAAAAAAAQAAAQ.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "class PetSegmentationPointPrompt:\n",
    "    def __init__(self, model_type=\"vit_h\", checkpoint_path=\"../model/sam_checkpoints/sam_vit_h_4b8939.pth\"):\n",
    "        # Device configuration\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load SAM model\n",
    "        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        self.sam.to(device=self.device)\n",
    "        \n",
    "        # Initialize predictor\n",
    "        self.predictor = SamPredictor(self.sam)\n",
    "\n",
    "    def extract_pet_mask(self, image_path):\n",
    "        \"\"\"\n",
    "        Extract pet mask using center point prompt\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Set image in predictor\n",
    "        self.predictor.set_image(image_rgb)\n",
    "        \n",
    "        # Get image center point\n",
    "        height, width = image.shape[:2]\n",
    "        center_point = np.array([[width//2, height//2]])\n",
    "        \n",
    "        # Generate mask from point prompt\n",
    "        point_label = np.array([1])  # 1 indicates foreground point\n",
    "        \n",
    "        masks, scores, logits = self.predictor.predict(\n",
    "            point_coords=center_point,\n",
    "            point_labels=point_label,\n",
    "            multimask_output=True\n",
    "        )\n",
    "        \n",
    "        areas = np.sum(masks, axis=(1, 2))\n",
    "        # Combine areas and scores for ranking\n",
    "        # Normalize areas to 0-1 range\n",
    "        normalized_areas = (areas - np.min(areas)) / (np.max(areas) - np.min(areas))\n",
    "        # Combine normalized areas and scores\n",
    "        area_wight = 0.6\n",
    "        combined_scores = area_wight * normalized_areas + (1 - area_wight) * scores\n",
    "        # Select best mask based on score\n",
    "        best_mask_idx = np.argmax(combined_scores)\n",
    "        best_mask = masks[best_mask_idx]\n",
    "        \n",
    "        # Create visualization\n",
    "        mask_visualization = self.create_mask_visualization(image_rgb, best_mask)\n",
    "        \n",
    "        return best_mask, mask_visualization\n",
    "    \n",
    "    def create_mask_visualization(self, image, mask):\n",
    "        visualization = image.copy()\n",
    "        color = np.array([30, 144, 255])  # Dodger Blue\n",
    "        \n",
    "        mask_overlay = visualization.copy()\n",
    "        mask_overlay[mask] = color\n",
    "        \n",
    "        alpha = 0.5\n",
    "        visualization = cv2.addWeighted(visualization, 1 - alpha, mask_overlay, alpha, 0)\n",
    "        return visualization\n",
    "\n",
    "    def process_demo_dataset(self, input_dir, output_dir, num_images=10):\n",
    "        # Create output directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'masks'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'visualizations'), exist_ok=True)\n",
    "        \n",
    "        # Get image files\n",
    "        image_files = [f for f in os.listdir(input_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        demo_files = image_files[:num_images]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for filename in demo_files:\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Extract pet mask using point prompt\n",
    "                pet_mask, mask_visualization = self.extract_pet_mask(input_path)\n",
    "                \n",
    "                if pet_mask is not None:\n",
    "                    # Original image\n",
    "                    image = cv2.imread(input_path)\n",
    "                    \n",
    "                    # Create masked image\n",
    "                    masked_image = image.copy()\n",
    "                    masked_image[~pet_mask] = [255, 255, 255]  # White background\n",
    "                    \n",
    "                    # Save outputs\n",
    "                    mask_output_path = os.path.join(output_dir, 'masks', filename)\n",
    "                    cv2.imwrite(mask_output_path, (pet_mask * 255).astype(np.uint8))\n",
    "                    \n",
    "                    masked_image_path = os.path.join(output_dir, filename)\n",
    "                    cv2.imwrite(masked_image_path, masked_image)\n",
    "                    \n",
    "                    viz_path = os.path.join(output_dir, 'visualizations', filename)\n",
    "                    cv2.imwrite(viz_path, cv2.cvtColor(mask_visualization, cv2.COLOR_RGB2BGR))\n",
    "                    \n",
    "                    results.append({\n",
    "                        'filename': filename,\n",
    "                        'mask_path': mask_output_path,\n",
    "                        'masked_image_path': masked_image_path,\n",
    "                        'visualization_path': viz_path\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Processed {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Initialize segmentor\n",
    "    segmentor = PetSegmentationPointPrompt()\n",
    "    \n",
    "    # Process demo dataset\n",
    "    demo_results = segmentor.process_demo_dataset(\n",
    "        input_dir='../dataset/trainset',\n",
    "        output_dir='../dataset/preprocessed_trainset_point_prompt_demo',\n",
    "        num_images=15\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
